\section{Description of the PLS  algorithm}

\begin{frame}[fragile]
	\frametitle{NIPALS algorithm}
	The most popular algorithm used in PLS to compute the model parameters is known as \textbf{non-iterative partial least squares} (\textbf{NIPALS}). There are two versions of this technique:
	\begin{itemize}
		\item \textbf{PLS1}: each of the \textit{p} predicted variables in modeled separately, resulting in one model for each class;
		\item \textbf{PLS2}: all predicted variables are modeled simultaneously.
	\end{itemize}
	The first algorithm is more accurate than the other, however it requires more computational time than PLS2 to find the $\alpha$ eigenvectors into which project the \textit{m} covariates. 
\end{frame}

\begin{frame}
	\frametitle{Data structures}
	Before starting with the description of the algorithm, we recall that:
	\begin{itemize}
		\item the matrix $X \in \mathbb{R}^{n\times m}$ is decomposed into a \textbf{score matrix} $T \in \mathbb{R}^{n\times\alpha}$ and a \textbf{loading matrix} $P \in \mathbb{R}^{m\times\alpha}$ such that $X = \hat{X} + E = T\cdot P^\top + E$, where $E \in \mathbb{R}^{n\times m}$ is the (true) \textbf{residual} matrix for $X$;
		\item the matrix $Y \in \mathbb{R}^{n\times p}$ is decomposed into a \textbf{score matrix} $U\in\mathbb{R}^{n\times\alpha}$ and a \textbf{loading matrix} $Q\in \mathbb{R}^{p\times\alpha}$ such that $Y = \hat{Y} + \widetilde{F} = U\cdot Q^\top + \widetilde{F}$, where $\widetilde{F}\in \mathbb{R}^{n\times p}$ is the (true) \textbf{residual matrix} for $Y$.
		\item the matrix $B\in \mathbb{R}^{\alpha\times\alpha}$ is the \textbf{diagonal regression matrix} such that $\hat{U} = T\cdot B$.
	\end{itemize}
	Therefore:
	\begin{center}
		$Y = \hat{U}\cdot Q^\top + F =  T\cdot B\cdot Q^\top + F$
	\end{center}
	where $F$ is the \textbf{prediction error matrix}; $B$ is selected such that the induced $2$-norm of $F$ is minimized. 
\end{frame}

\begin{frame}[fragile]
	\frametitle{MATLAB code}
	The following MATLAB code implements the PLS2 algorithm:
	\begin{Verbatim}[tabsize=4, commandchars=\\\{\}, frame=topline]
E = X; \textcolor{green}{% residual matrix for X}
F = Y; \textcolor{green}{% residual matrix for Y}
[~, idx] = max(sum(Y.*Y));
\textcolor{green}{% search of the j-th eigenvector}
\textcolor{blue}{for} j = 1:alpha
	u = F(:, idx);
	tOld = 0;
	\textcolor{blue}{for} i = 1:maxIter
		w = (E'*u)/norm(E'*u); \textcolor{green}{% support vector}
		t = E*w; \textcolor{green}{% j-th column of the score matrix for X}
		q = (F'*t)/norm(F'*t); \textcolor{green}{% j-th column of the...}
			\textcolor{green}{% loading matrix for Y}
		u = F*q; \textcolor{green}{% j-th column of the score matrix for Y}
	\end{Verbatim}
\end{frame}

\begin{frame}[fragile]
	\begin{Verbatim}[tabsize=4, commandchars=\\\{\}]
		\textcolor{blue}{if} abs(tOld - t) < exitTol
			\textcolor{blue}{break};
		\textcolor{blue}{else}
			tOld = t;
		\textcolor{blue}{end}
	\textcolor{blue}{end}
	p = (E'*t)/(t'*t); \textcolor{green}{% j-th column of the...}
		\textcolor{green}{% loading matrix of X}
	\textcolor{green}{% scaling}
	t = t*norm(p);
	w = w*norm(p);
	p = p/norm(p);
	\textcolor{green}{% calculation of b and the error matrices}
	b = (u'*t)/(t'*t); \textcolor{green}{% j-th column of the...}
	    \textcolor{green}{% coefficient regression matrix}
	E = E - t*p';  \textcolor{green}{% update of the residuals for matrix X}
	F = F - b*t*q'; \textcolor{green}{% update of the residuals for matrix Y}
	\end{Verbatim}
\end{frame}

\begin{frame}[fragile]
	\begin{Verbatim}[tabsize=4, commandchars=\\\{\}, frame=bottomline]
	\textcolor{green}{% calculation of W, P, T and B2}
	W(:, j) = w;
	P(:, j) = p;
	T(:, j) = t;
	B2 = W*(P'*W)^-1*(T'*T)^-1*T'*Y;
\textcolor{blue}{end}
Y_hat = X*B2; \textcolor{green}{% computation of predictions}
	\end{Verbatim}
For each row of \verb|Y_hat| the fault class is chosen by assigning $1$ to the column whose value si greater than that of the others, $0$ otherwise. \\Moreover, to increase the performances of PLS it is necessary \textbf{normalize} both $X$ and $Y$ before running the algorithm.
\end{frame}
