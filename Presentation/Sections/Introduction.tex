\section{Introduction}

\begin{frame}
	\frametitle{Introduction to the PLS technique}
	\textbf{Partial least squares} (\textbf{PLS}), as known as \textbf{projection to latent structures}, is a dimensionality reduction technique for maximizing the \textbf{covariance} between the predictor (independent) matrix $X \in \mathbb{R}^{n \times m}$ and the predicted (dependent) matrix $Y \in \mathbb{R}^{n \times p}$ for each component of the reduced space $\mathbb{R}^\alpha$ with $\alpha \le m$, where:
	\begin{itemize}
		\item $n$ = number of observations;
		\item $m$ = number of covariates (input variables);
		\item $p$ = number of dependent variables (output variables);
		\item $\alpha$ = dimension of the reduced space in which $X$ is projected.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Popular application of PLS}
	This technique is often used in \textbf{fault detection} and \textbf{isolation}. With PLS is possible to treat both regression and classification problems. The matrix $X$ always contains the process variables (e.g. diameter and thickness of a gasket), while the matrix $Y$ only (quantitative) quality variables (e.g. its mechanical seal) in the regression case, whereas in pattern classification the predicted variables are dummy variables ($1$ or $0$) such as:
	\begin{equation}
		Y = 
		\begin{bmatrix}
			1 & \dots & 1 & 0 & \dots & 0 & 0 & \dots & 0\\
			0 & \dots & 0 & 1 & \dots & 1 & 0 & \dots & 0\\
			0 & \dots & 0 & 0 & \dots & 0 & 1 & \dots & 1\\
		\end{bmatrix}^\top
	\end{equation}
	where each column of $Y$ corresponds to a fault class. The first $n_j$ elements of column $j$ are filled with a $1$, which indicates that the first $n_j$ rows of $X$ are data from fault $j$. In this case PLS is called \textbf{discriminant}.
\end{frame}